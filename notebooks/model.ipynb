{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForRE(TFBertPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.ent_dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "\n",
    "        pooled_output = outputs[1] # last layer hidden-state of the first token of the sequence [CLS] (batch_size, hidden_size)\n",
    "        sequence_output = outputs[0] # sequence of hidden-states at the output of the last layer of the model (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        def extract_entity(sequence_output, e_mask):\n",
    "            extended_e_mask = tf.expand_dims(e_mask, 1) # shape (batch_size, 1, sequence_length)\n",
    "            ext_entity = tf.matmul(extended_e_mask, sequence_output) # shape (batch_size, 1, hidden_size)\n",
    "            return tf.squeeze(ext_entity, [1]) # shape (batch_size, hidden_size)\n",
    "\n",
    "        e1_mask = inputs.get(\"e1_mask\", e1_mask)\n",
    "        e2_mask = inputs.get(\"e2_mask\", e2_mask)\n",
    "\n",
    "        e1_h = self.ent_dropout(extract_entity(sequence_output, e1_mask))\n",
    "        e2_h = self.ent_dropout(extract_entity(sequence_output, e2_mask))\n",
    "        context = self.dropout(pooled_output, training=kwargs.get(\"training\", False))\n",
    "        # now concatenate 3 tensors:\n",
    "        pooled_output = tf.concat([context,e1_h,e2_h], -1)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}