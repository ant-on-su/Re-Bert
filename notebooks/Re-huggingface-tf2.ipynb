{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction (SemEval task 8) with Huggingface transformers & TF2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task - predict semantic relations between pairs of nominals.<br>\n",
    "**There are 9 (directional) types:**\n",
    "- Cause-Effect\n",
    "- Component-Whole\n",
    "- Content-Container\n",
    "- Entity-Destination\n",
    "- Entity-Origin\n",
    "- Instrument-Agency\n",
    "- Member-Collection\n",
    "- Message-Topic\n",
    "- Product-Producer\n",
    "- *Other* (none of above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semeval_processor\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/tuned_bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(text):\n",
    "    encoder = semeval_processor.Semeval()\n",
    "    tokens_in = encoder.re_encode(input_text=text,\n",
    "                                  tokenizer=tokenizer)\n",
    "    pred = model.predict(tokens_in)\n",
    "    \n",
    "    return encoder.re_decode(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find RE between 2 entities in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.5 ms, sys: 13.5 ms, total: 95 ms\n",
      "Wall time: 84.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Component-Whole(e2,e1)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The <e1>dataset</e1> contains nine semantic relation types and one artificial relation type <e2>Other</e2>, \\\n",
    "            which means that the relation does not belong to any of the nine relation types.\"\n",
    "\n",
    "%time get_relation(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SemEval-2010 Task 8 dataset: <br>\n",
    "contains 8000 train and 2717 test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import semeval_processor\n",
    "from utils import features_from_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames for the input and tmp data:\n",
    "train_original = '../data/TRAIN_FILE.TXT'\n",
    "test_original = '../data/TEST_FILE.txt'\n",
    "\n",
    "train_tsv = '../data/train_m.tsv'\n",
    "test_tsv = '../data/test.tsv'\n",
    "\n",
    "pickle_features = '../data/train_features'\n",
    "pickle_test = '../data/test_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features_from_tsv(train_tsv, tokenizer)\n",
    "test_features = features_from_tsv(test_tsv, tokenizer, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle and unpickle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the train and test features to pickle: \n",
    "pickle.dump(train_features,open(pickle_features,'wb'))\n",
    "pickle.dump(test_features,open(pickle_test,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test features from pickle\n",
    "train_features = pickle.load(open(pickle_features,'rb'))\n",
    "test_features = pickle.load(open(pickle_test,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ids, xtest_ids, xtrain_mask, xtest_mask, xtrain_e1, xtest_e1, xtrain_e2, xtest_e2, ytrain, ytest = train_test_split(\n",
    "    train_features['input_ids'],\n",
    "    train_features['attention_mask'],\n",
    "    train_features['e1_mask'],\n",
    "    train_features['e2_mask'],\n",
    "    train_features['label_id'],\n",
    "    stratify = train_features['label_id'],\n",
    "    test_size=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "Let's get BERT base uncased model and try to fine-tune it to our downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'bert-base-uncased'\n",
    "config = BertConfig()\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFBertModel.from_pretrained(pretrained_model, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom layer to extract the embeddings vectors of our marked entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Extract_entity(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Extract_entity, self).__init__()\n",
    "    \n",
    "    def call(self, sequence_output, e_mask):\n",
    "        extended_e_mask = tf.expand_dims(e_mask, 1) # shape (batch_size, 1, sequence_length)\n",
    "        extended_e_mask = tf.cast(extended_e_mask, tf.float32)\n",
    "        ext_entity = tf.matmul(extended_e_mask, sequence_output) # shape (batch_size, 1, hidden_size)\n",
    "        \n",
    "        return tf.squeeze(ext_entity, [1]) # shape (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup tensorboard for watching some nice pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = \"../.logs\"\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    import os\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and add callbacks for early stopping, and reducing learning rate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "lr_on_pla = tf.keras.callbacks.ReduceLROnPlateau(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned BERT model\n",
    "\n",
    "Let's use embeddings (the top layer) from the pretrained hugginface Bert and add dense layrs on top, as in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_tuned(dropout_rate):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(128,), name='e1_mask', dtype='int32')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(128,), name='e2_mask', dtype='int32')\n",
    "\n",
    "    embedding_layer = transformer_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_h = embedding_layer[:,0,:]\n",
    "    e1_h = Extract_entity()(embedding_layer,e1_mask)\n",
    "    e2_h = Extract_entity()(embedding_layer,e2_mask)\n",
    "    CLS = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='cls_dense')(cls_h)\n",
    "    CLS = tf.keras.layers.Dropout(dropout_rate)(CLS)\n",
    "    entity_layer = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='ent')\n",
    "    E1 = entity_layer(e1_h)\n",
    "    E1 = tf.keras.layers.Dropout(dropout_rate)(E1)\n",
    "    E2 = entity_layer(e2_h)\n",
    "    E2 = tf.keras.layers.Dropout(dropout_rate)(E2)\n",
    "    X = tf.concat([CLS,E1,E2],-1)\n",
    "    X = tf.keras.layers.Dense(config.hidden_size*3, activation=\"elu\", kernel_initializer=\"he_normal\")(X)\n",
    "    X = tf.keras.layers.Dropout(dropout_rate)(X)\n",
    "    X = tf.keras.layers.Dense(19, activation='softmax')(X)\n",
    "    model = tf.keras.Model(inputs=[input_ids,attention_mask,e1_mask,e2_mask], outputs=[X])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "e1_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e2_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity (Extract_entity) (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_1 (Extract_entit (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense (Dense)               (None, 768)          590592      tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "ent (Dense)                     (None, 768)          590592      extract_entity[0][0]             \n",
      "                                                                 extract_entity_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           cls_dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 768)          0           ent[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 768)          0           ent[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 2304)]       0           dropout_37[0][0]                 \n",
      "                                                                 dropout_38[0][0]                 \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2304)         5310720     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 2304)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 19)           43795       dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 116,017,939\n",
      "Trainable params: 116,017,939\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tuned_bert = create_model_tuned(dropout_rate=0.2)\n",
    "model_tuned_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1601 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "  16/6400 [..............................] - ETA: 1:06:30 - loss: 4.1324 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.949269). Check your callbacks.\n",
      "6400/6400 [==============================] - 185s 29ms/sample - loss: 1.2393 - accuracy: 0.6147 - val_loss: 0.6283 - val_accuracy: 0.8145\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - 171s 27ms/sample - loss: 0.4164 - accuracy: 0.8708 - val_loss: 0.6215 - val_accuracy: 0.8207\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 170s 27ms/sample - loss: 0.1770 - accuracy: 0.9425 - val_loss: 0.7194 - val_accuracy: 0.8301\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 170s 27ms/sample - loss: 0.0687 - accuracy: 0.9792 - val_loss: 0.7170 - val_accuracy: 0.8395\n",
      "Epoch 5/100\n",
      "6384/6400 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9939Restoring model weights from the end of the best epoch.\n",
      "6400/6400 [==============================] - 171s 27ms/sample - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.7249 - val_accuracy: 0.8445\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model_tuned_bert.fit([xtrain_ids, xtrain_mask, xtrain_e1, xtrain_e2],\n",
    "                   ytrain,\n",
    "                   validation_data = ([xtest_ids, xtest_mask, xtest_e1, xtest_e2], ytest),\n",
    "                   epochs=100,\n",
    "                   batch_size=16,\n",
    "                   callbacks=[early_stop,\n",
    "                             lr_on_pla,\n",
    "                             tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/tuned_bert/assets\n"
     ]
    }
   ],
   "source": [
    "model_tuned_bert.save('../models/tuned_bert',save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen BERT model\n",
    "\n",
    "Let's use BERT pre-trained model as the feature extractor by freezing Bert's output (embeddings). <br>\n",
    "In this way we'll be training only added dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_frozen(dropout_rate):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(128,), name='e1_mask', dtype='int32')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(128,), name='e2_mask', dtype='int32')\n",
    "\n",
    "    embedding_layer = transformer_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_h = embedding_layer[:,0,:]\n",
    "    e1_h = Extract_entity()(embedding_layer,e1_mask)\n",
    "    e2_h = Extract_entity()(embedding_layer,e2_mask)\n",
    "    CLS = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='cls_dense')(cls_h)\n",
    "    CLS = tf.keras.layers.Dropout(dropout_rate)(CLS)\n",
    "    entity_layer = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='ent')\n",
    "    E1 = entity_layer(e1_h)\n",
    "    E1 = tf.keras.layers.Dropout(dropout_rate)(E1)\n",
    "    E2 = entity_layer(e2_h)\n",
    "    E2 = tf.keras.layers.Dropout(dropout_rate)(E2)\n",
    "    X = tf.concat([CLS,E1,E2],-1)\n",
    "    X = tf.keras.layers.Dense(config.hidden_size*3, activation=\"elu\", kernel_initializer=\"he_normal\")(X)\n",
    "    X = tf.keras.layers.Dropout(dropout_rate)(X)\n",
    "    X = tf.keras.layers.Dense(19, activation='softmax')(X)\n",
    "    model = tf.keras.Model(inputs=[input_ids,attention_mask,e1_mask,e2_mask], outputs=[X])\n",
    "\n",
    "    #don't mess up BERT layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "e1_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e2_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_2 (Extract_entit (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_3 (Extract_entit (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense (Dense)               (None, 768)          590592      tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "ent (Dense)                     (None, 768)          590592      extract_entity_2[0][0]           \n",
      "                                                                 extract_entity_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 768)          0           cls_dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 768)          0           ent[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 768)          0           ent[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 2304)]       0           dropout_41[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2304)         5310720     tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 2304)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 19)           43795       dropout_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 116,017,939\n",
      "Trainable params: 6,535,699\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_frozen_bert = create_model_frozen(dropout_rate=0.15)\n",
    "model_frozen_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1601 samples\n",
      "Epoch 1/100\n",
      "6400/6400 [==============================] - 76s 12ms/sample - loss: 2.1052 - accuracy: 0.3575 - val_loss: 1.4154 - val_accuracy: 0.5665\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 1.2983 - accuracy: 0.5864 - val_loss: 1.1416 - val_accuracy: 0.6321\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 1.0108 - accuracy: 0.6711 - val_loss: 1.0277 - val_accuracy: 0.6590\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.8691 - accuracy: 0.7148 - val_loss: 0.9929 - val_accuracy: 0.6771\n",
      "Epoch 5/100\n",
      "6400/6400 [==============================] - 69s 11ms/sample - loss: 0.7598 - accuracy: 0.7503 - val_loss: 0.9425 - val_accuracy: 0.6889\n",
      "Epoch 6/100\n",
      "6400/6400 [==============================] - 69s 11ms/sample - loss: 0.6937 - accuracy: 0.7684 - val_loss: 0.9416 - val_accuracy: 0.6996\n",
      "Epoch 7/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.6304 - accuracy: 0.7858 - val_loss: 0.9047 - val_accuracy: 0.7096\n",
      "Epoch 8/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.5819 - accuracy: 0.8070 - val_loss: 0.9118 - val_accuracy: 0.7021\n",
      "Epoch 9/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.5458 - accuracy: 0.8180 - val_loss: 0.8652 - val_accuracy: 0.7227\n",
      "Epoch 10/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.5146 - accuracy: 0.8236 - val_loss: 0.8770 - val_accuracy: 0.7208\n",
      "Epoch 11/100\n",
      "6400/6400 [==============================] - 71s 11ms/sample - loss: 0.4707 - accuracy: 0.8363 - val_loss: 0.8728 - val_accuracy: 0.7302\n",
      "Epoch 12/100\n",
      "6400/6400 [==============================] - 71s 11ms/sample - loss: 0.4161 - accuracy: 0.8612 - val_loss: 0.8623 - val_accuracy: 0.7270\n",
      "Epoch 13/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.4061 - accuracy: 0.8664 - val_loss: 0.8583 - val_accuracy: 0.7295\n",
      "Epoch 14/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.4029 - accuracy: 0.8656 - val_loss: 0.8606 - val_accuracy: 0.7295\n",
      "Epoch 15/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.4053 - accuracy: 0.8664 - val_loss: 0.8581 - val_accuracy: 0.7295\n",
      "Epoch 16/100\n",
      "6400/6400 [==============================] - 69s 11ms/sample - loss: 0.3883 - accuracy: 0.8730 - val_loss: 0.8579 - val_accuracy: 0.7270\n",
      "Epoch 17/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.3859 - accuracy: 0.8711 - val_loss: 0.8562 - val_accuracy: 0.7283\n",
      "Epoch 18/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.3888 - accuracy: 0.8695 - val_loss: 0.8548 - val_accuracy: 0.7333\n",
      "Epoch 19/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.3762 - accuracy: 0.8731 - val_loss: 0.8547 - val_accuracy: 0.7302\n",
      "Epoch 20/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.3835 - accuracy: 0.8745 - val_loss: 0.8542 - val_accuracy: 0.7289\n",
      "Epoch 21/100\n",
      "6400/6400 [==============================] - 69s 11ms/sample - loss: 0.3679 - accuracy: 0.8813 - val_loss: 0.8526 - val_accuracy: 0.7283\n",
      "Epoch 22/100\n",
      "6400/6400 [==============================] - 71s 11ms/sample - loss: 0.3700 - accuracy: 0.8770 - val_loss: 0.8495 - val_accuracy: 0.7295\n",
      "Epoch 23/100\n",
      "6400/6400 [==============================] - 71s 11ms/sample - loss: 0.3604 - accuracy: 0.8828 - val_loss: 0.8482 - val_accuracy: 0.7327\n",
      "Epoch 24/100\n",
      "6400/6400 [==============================] - 71s 11ms/sample - loss: 0.3606 - accuracy: 0.8855 - val_loss: 0.8498 - val_accuracy: 0.7364\n",
      "Epoch 25/100\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.3616 - accuracy: 0.8783 - val_loss: 0.8503 - val_accuracy: 0.7327\n",
      "Epoch 26/100\n",
      "6384/6400 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8819Restoring model weights from the end of the best epoch.\n",
      "6400/6400 [==============================] - 70s 11ms/sample - loss: 0.3604 - accuracy: 0.8822 - val_loss: 0.8496 - val_accuracy: 0.7327\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model_frozen_bert.fit([xtrain_ids, xtrain_mask, xtrain_e1, xtrain_e2],\n",
    "                   ytrain,\n",
    "                   validation_data = ([xtest_ids, xtest_mask, xtest_e1, xtest_e2], ytest),\n",
    "                   epochs=100,\n",
    "                   batch_size=16,\n",
    "                   callbacks=[early_stop,\n",
    "                             lr_on_pla]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/frozen_bert/assets\n"
     ]
    }
   ],
   "source": [
    "model_frozen_bert.save('../models/frozen_bert',save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate Models\n",
    "\n",
    "get results for the test set and store in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_semeval_result(result_file, prediction):\n",
    "\n",
    "    res = [semeval_processor.Semeval().RELATION_LABELS[x.argmax()] for x in prediction]\n",
    "\n",
    "    with open(result_file, \"w\") as f:\n",
    "        for i, answer in enumerate(res):\n",
    "            f.write(f\"{8001+i}\\t{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned BERT\n",
    "pred_res = model_tuned_bert.predict([\n",
    "                            test_features['input_ids'],\n",
    "                            test_features['attention_mask'],\n",
    "                            test_features['e1_mask'],\n",
    "                            test_features['e2_mask']\n",
    "                            ])\n",
    "\n",
    "write_semeval_result('../results/model_tuned_bert.txt', pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frozen BERT\n",
    "pred_res = model_frozen_bert.predict([\n",
    "                            test_features['input_ids'],\n",
    "                            test_features['attention_mask'],\n",
    "                            test_features['e1_mask'],\n",
    "                            test_features['e2_mask']\n",
    "                            ])\n",
    "\n",
    "write_semeval_result('../results/model_frozen_bert.txt', pred_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the outcome \n",
    "\n",
    "using original evaluation script (looong and detailed)\n",
    "\n",
    "#### Tuned BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/results\n",
      "<<< (2*9+1)-WAY EVALUATION (USING DIRECTIONALITY)>>>:\n",
      "\n",
      "Confusion matrix:\n",
      "        C-E1 C-E2 C-W1 C-W2 C-C1 C-C2 E-D1 E-D2 E-O1 E-O2 I-A1 I-A2 M-C1 M-C2 M-T1 M-T2 P-P1 P-P2  _O_ <-- classified as\n",
      "      +-----------------------------------------------------------------------------------------------+ -SUM- skip ACTUAL\n",
      " C-E1 | 128    1    0    0    0    0    0    0    0    0    0    0    0    0    2    0    0    2    1 |  134    0  134\n",
      " C-E2 |   0  183    0    0    0    0    0    0    3    0    0    0    0    0    0    0    0    0    8 |  194    0  194\n",
      " C-W1 |   0    0  141    7    1    0    0    0    0    0    1    0    1    2    1    0    0    1    7 |  162    0  162\n",
      " C-W2 |   0    0    3  122    0    1    0    0    1    0    0    7    0    2    4    0    0    3    7 |  150    0  150\n",
      " C-C1 |   0    0    6    1  127    0   13    0    1    0    0    0    0    0    0    0    0    0    5 |  153    0  153\n",
      " C-C2 |   0    0    0    4    0   32    0    0    0    1    0    0    0    1    0    0    0    1    0 |   39    0   39\n",
      " E-D1 |   0    0    3    0    0    0  272    0    0    0    0    0    0    0    1    2    0    0   13 |  291    0  291\n",
      " E-D2 |   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1 |    1    0    1\n",
      " E-O1 |   0    5    5    1    1    0    3    0  171    0    0    1    0    0    0    1    4    0   19 |  211    0  211\n",
      " E-O2 |   2    0    0    0    0    0    0    0    0   40    0    0    0    0    0    0    0    4    1 |   47    0   47\n",
      " I-A1 |   0    0    0    0    0    0    0    0    0    0   14    0    0    0    0    0    1    0    7 |   22    0   22\n",
      " I-A2 |   0    0    0    4    0    0    0    0    0    0    0  110    0    0    2    0    0    4   14 |  134    0  134\n",
      " M-C1 |   0    0    0    0    0    0    1    0    1    0    0    0   21    1    0    1    0    1    6 |   32    0   32\n",
      " M-C2 |   0    0    2    0    0    0    0    0    0    0    0    1    0  178    0    0    0    1   19 |  201    0  201\n",
      " M-T1 |   0    1    0    0    0    0    0    0    0    0    0    0    0    1  203    0    0    0    5 |  210    0  210\n",
      " M-T2 |   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   51    0    0    0 |   51    0   51\n",
      " P-P1 |   0    4    0    0    0    0    0    0    3    0    0    0    0    0    1    0   94    1    5 |  108    0  108\n",
      " P-P2 |   1    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0  118    3 |  123    0  123\n",
      "  _O_ |  11   13   23   13   10    2   16    0   15    3    2   17    3   18   37   12   10   21  228 |  454    0  454\n",
      "      +-----------------------------------------------------------------------------------------------+\n",
      " -SUM-  142  207  183  152  139   35  305    0  195   44   17  137   25  203  251   67  109  157  349   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Other) = 2233/2717 = 82.19%\n",
      "\n",
      "Results for the individual relations:\n",
      "      Cause-Effect(e1,e2) :    P =  128/ 142 =  90.14%     R =  128/ 134 =  95.52%     F1 =  92.75%\n",
      "      Cause-Effect(e2,e1) :    P =  183/ 207 =  88.41%     R =  183/ 194 =  94.33%     F1 =  91.27%\n",
      "   Component-Whole(e1,e2) :    P =  141/ 183 =  77.05%     R =  141/ 162 =  87.04%     F1 =  81.74%\n",
      "   Component-Whole(e2,e1) :    P =  122/ 152 =  80.26%     R =  122/ 150 =  81.33%     F1 =  80.79%\n",
      " Content-Container(e1,e2) :    P =  127/ 139 =  91.37%     R =  127/ 153 =  83.01%     F1 =  86.99%\n",
      " Content-Container(e2,e1) :    P =   32/  35 =  91.43%     R =   32/  39 =  82.05%     F1 =  86.49%\n",
      "Entity-Destination(e1,e2) :    P =  272/ 305 =  89.18%     R =  272/ 291 =  93.47%     F1 =  91.28%\n",
      "Entity-Destination(e2,e1) :    P =    0/   0 =   0.00%     R =    0/   1 =   0.00%     F1 =   0.00%\n",
      "     Entity-Origin(e1,e2) :    P =  171/ 195 =  87.69%     R =  171/ 211 =  81.04%     F1 =  84.24%\n",
      "     Entity-Origin(e2,e1) :    P =   40/  44 =  90.91%     R =   40/  47 =  85.11%     F1 =  87.91%\n",
      " Instrument-Agency(e1,e2) :    P =   14/  17 =  82.35%     R =   14/  22 =  63.64%     F1 =  71.79%\n",
      " Instrument-Agency(e2,e1) :    P =  110/ 137 =  80.29%     R =  110/ 134 =  82.09%     F1 =  81.18%\n",
      " Member-Collection(e1,e2) :    P =   21/  25 =  84.00%     R =   21/  32 =  65.62%     F1 =  73.68%\n",
      " Member-Collection(e2,e1) :    P =  178/ 203 =  87.68%     R =  178/ 201 =  88.56%     F1 =  88.12%\n",
      "     Message-Topic(e1,e2) :    P =  203/ 251 =  80.88%     R =  203/ 210 =  96.67%     F1 =  88.07%\n",
      "     Message-Topic(e2,e1) :    P =   51/  67 =  76.12%     R =   51/  51 = 100.00%     F1 =  86.44%\n",
      "  Product-Producer(e1,e2) :    P =   94/ 109 =  86.24%     R =   94/ 108 =  87.04%     F1 =  86.64%\n",
      "  Product-Producer(e2,e1) :    P =  118/ 157 =  75.16%     R =  118/ 123 =  95.93%     F1 =  84.29%\n",
      "                   _Other :    P =  228/ 349 =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2005/2368 =  84.67%     R = 2005/2263 =  88.60%     F1 =  86.59%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  79.95%\tR =  81.25%\tF1 =  80.20%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION IGNORING DIRECTIONALITY >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- skip ACTUAL\n",
      "  C-E | 312    0    0    0    3    0    0    2    2    9 |  328    0  328\n",
      "  C-W |   0  273    2    0    1    8    5    5    4   14 |  312    0  312\n",
      "  C-C |   0   11  159   13    2    0    1    0    1    5 |  192    0  192\n",
      "  E-D |   0    3    0  272    0    0    0    3    0   14 |  292    0  292\n",
      "  E-O |   7    6    1    3  211    1    0    1    8   20 |  258    0  258\n",
      "  I-A |   0    4    0    0    0  124    0    2    5   21 |  156    0  156\n",
      "  M-C |   0    2    0    1    1    1  200    1    2   25 |  233    0  233\n",
      "  M-T |   1    0    0    0    0    0    1  254    0    5 |  261    0  261\n",
      "  P-P |   5    0    0    0    3    1    0    1  213    8 |  231    0  231\n",
      "  _O_ |  24   36   12   16   18   19   21   49   31  228 |  454    0  454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  349  335  174  305  239  154  228  318  266  349   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2246/2717 = 82.66%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2246/2717 = 82.66%\n",
      "Accuracy (considering all skipped examples as Other) = 2246/2717 = 82.66%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  312/ 349 =  89.40%     R =  312/ 328 =  95.12%     F1 =  92.17%\n",
      "          Component-Whole :    P =  273/ 335 =  81.49%     R =  273/ 312 =  87.50%     F1 =  84.39%\n",
      "        Content-Container :    P =  159/ 174 =  91.38%     R =  159/ 192 =  82.81%     F1 =  86.89%\n",
      "       Entity-Destination :    P =  272/ 305 =  89.18%     R =  272/ 292 =  93.15%     F1 =  91.12%\n",
      "            Entity-Origin :    P =  211/ 239 =  88.28%     R =  211/ 258 =  81.78%     F1 =  84.91%\n",
      "        Instrument-Agency :    P =  124/ 154 =  80.52%     R =  124/ 156 =  79.49%     F1 =  80.00%\n",
      "        Member-Collection :    P =  200/ 228 =  87.72%     R =  200/ 233 =  85.84%     F1 =  86.77%\n",
      "            Message-Topic :    P =  254/ 318 =  79.87%     R =  254/ 261 =  97.32%     F1 =  87.74%\n",
      "         Product-Producer :    P =  213/ 266 =  80.08%     R =  213/ 231 =  92.21%     F1 =  85.71%\n",
      "                   _Other :    P =  228/ 349 =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2018/2368 =  85.22%     R = 2018/2263 =  89.17%     F1 =  87.15%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  85.32%\tR =  88.36%\tF1 =  86.63%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- xDIRx skip  ACTUAL\n",
      "  C-E | 311    0    0    0    3    0    0    2    2    9 |  327     1     0    328\n",
      "  C-W |   0  263    2    0    1    8    5    5    4   14 |  302    10     0    312\n",
      "  C-C |   0   11  159   13    2    0    1    0    1    5 |  192     0     0    192\n",
      "  E-D |   0    3    0  272    0    0    0    3    0   14 |  292     0     0    292\n",
      "  E-O |   7    6    1    3  211    1    0    1    8   20 |  258     0     0    258\n",
      "  I-A |   0    4    0    0    0  124    0    2    5   21 |  156     0     0    156\n",
      "  M-C |   0    2    0    1    1    1  199    1    2   25 |  232     1     0    233\n",
      "  M-T |   1    0    0    0    0    0    1  254    0    5 |  261     0     0    261\n",
      "  P-P |   5    0    0    0    3    1    0    1  212    8 |  230     1     0    231\n",
      "  _O_ |  24   36   12   16   18   19   21   49   31  228 |  454     0     0    454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  348  325  174  305  239  154  227  318  265  349   2704    13     0   2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Other) = 2233/2717 = 82.19%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  311/( 348 +   1) =  89.11%     R =  311/ 328 =  94.82%     F1 =  91.88%\n",
      "          Component-Whole :    P =  263/( 325 +  10) =  78.51%     R =  263/ 312 =  84.29%     F1 =  81.30%\n",
      "        Content-Container :    P =  159/( 174 +   0) =  91.38%     R =  159/ 192 =  82.81%     F1 =  86.89%\n",
      "       Entity-Destination :    P =  272/( 305 +   0) =  89.18%     R =  272/ 292 =  93.15%     F1 =  91.12%\n",
      "            Entity-Origin :    P =  211/( 239 +   0) =  88.28%     R =  211/ 258 =  81.78%     F1 =  84.91%\n",
      "        Instrument-Agency :    P =  124/( 154 +   0) =  80.52%     R =  124/ 156 =  79.49%     F1 =  80.00%\n",
      "        Member-Collection :    P =  199/( 227 +   1) =  87.28%     R =  199/ 233 =  85.41%     F1 =  86.33%\n",
      "            Message-Topic :    P =  254/( 318 +   0) =  79.87%     R =  254/ 261 =  97.32%     F1 =  87.74%\n",
      "         Product-Producer :    P =  212/( 265 +   1) =  79.70%     R =  212/ 231 =  91.77%     F1 =  85.31%\n",
      "                   _Other :    P =  228/( 349 +   0) =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2005/2368 =  84.67%     R = 2005/2263 =  88.60%     F1 =  86.59%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  84.87%\tR =  87.87%\tF1 =  86.16%\n",
      "\n",
      "\n",
      "\n",
      "<<< The official score is (9+1)-way evaluation with directionality taken into account: macro-averaged F1 = 86.16% >>>\n",
      "/root/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../results\n",
    "!perl ../results/semeval2010_task8_scorer-v1.2.pl model_tuned_bert.txt test_keys.txt\n",
    "%cd ../notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozen BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/results\n",
      "<<< (2*9+1)-WAY EVALUATION (USING DIRECTIONALITY)>>>:\n",
      "\n",
      "Confusion matrix:\n",
      "        C-E1 C-E2 C-W1 C-W2 C-C1 C-C2 E-D1 E-D2 E-O1 E-O2 I-A1 I-A2 M-C1 M-C2 M-T1 M-T2 P-P1 P-P2  _O_ <-- classified as\n",
      "      +-----------------------------------------------------------------------------------------------+ -SUM- skip ACTUAL\n",
      " C-E1 | 113    7    0    1    0    0    0    0    0    0    1    0    0    0    3    0    1    1    7 |  134    0  134\n",
      " C-E2 |   1  175    1    0    0    0    0    0    2    0    0    0    0    0    0    1    0    0   14 |  194    0  194\n",
      " C-W1 |   1    0  130    6    0    0    3    0    1    0    2    0    0    3    0    0    0    0   16 |  162    0  162\n",
      " C-W2 |   0    0    7  118    0    2    0    0    1    0    0    1    0    1    4    0    0    0   16 |  150    0  150\n",
      " C-C1 |   0    0    5    0  131    1    9    0    2    0    0    0    0    0    0    0    1    0    4 |  153    0  153\n",
      " C-C2 |   0    0    0    3    2   28    0    0    0    1    0    0    0    1    1    0    0    0    3 |   39    0   39\n",
      " E-D1 |   0    1    5    1    9    0  237    0    4    0    1    1    0    0    1    1    0    0   30 |  291    0  291\n",
      " E-D2 |   0    0    0    0    0    1    0    0    0    0    0    0    0    0    0    0    0    0    0 |    1    0    1\n",
      " E-O1 |   0    4    2    3    3    0   17    0  139    0    0    1    1    0    1    2    8    0   30 |  211    0  211\n",
      " E-O2 |   0    0    0    1    0    0    0    0    0   40    0    0    0    0    0    0    0    2    4 |   47    0   47\n",
      " I-A1 |   0    0    1    0    0    0    0    0    0    0    9    0    0    0    0    0    5    0    7 |   22    0   22\n",
      " I-A2 |   0    1    0    5    0    0    1    0    0    0    0   88    0    0    2    0    0   11   26 |  134    0  134\n",
      " M-C1 |   0    0    1    0    0    0    0    0    1    0    0    0   15    0    0    1    0    0   14 |   32    0   32\n",
      " M-C2 |   0    1    1    3    0    1    0    0    0    0    2    0    0  177    0    0    0    0   16 |  201    0  201\n",
      " M-T1 |   1    2    0    1    0    0    1    0    0    0    0    0    0    1  181    1    0    0   22 |  210    0  210\n",
      " M-T2 |   0    0    0    1    0    0    0    0    1    0    0    0    0    0    0   40    0    1    8 |   51    0   51\n",
      " P-P1 |   0    4    0    0    0    0    0    0    6    0    2    0    0    2    1    0   86    1    6 |  108    0  108\n",
      " P-P2 |   0    0    1    2    0    0    2    0    0    1    0   10    0    1    0    0    0   85   21 |  123    0  123\n",
      "  _O_ |   9   14   13   20    9    4   26    0   21    2    1   13    5   30   23    8   17   16  223 |  454    0  454\n",
      "      +-----------------------------------------------------------------------------------------------+\n",
      " -SUM-  125  209  167  165  154   37  296    0  178   44   18  114   21  216  217   54  118  117  467   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2015/2717 = 74.16%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2015/2717 = 74.16%\n",
      "Accuracy (considering all skipped examples as Other) = 2015/2717 = 74.16%\n",
      "\n",
      "Results for the individual relations:\n",
      "      Cause-Effect(e1,e2) :    P =  113/ 125 =  90.40%     R =  113/ 134 =  84.33%     F1 =  87.26%\n",
      "      Cause-Effect(e2,e1) :    P =  175/ 209 =  83.73%     R =  175/ 194 =  90.21%     F1 =  86.85%\n",
      "   Component-Whole(e1,e2) :    P =  130/ 167 =  77.84%     R =  130/ 162 =  80.25%     F1 =  79.03%\n",
      "   Component-Whole(e2,e1) :    P =  118/ 165 =  71.52%     R =  118/ 150 =  78.67%     F1 =  74.92%\n",
      " Content-Container(e1,e2) :    P =  131/ 154 =  85.06%     R =  131/ 153 =  85.62%     F1 =  85.34%\n",
      " Content-Container(e2,e1) :    P =   28/  37 =  75.68%     R =   28/  39 =  71.79%     F1 =  73.68%\n",
      "Entity-Destination(e1,e2) :    P =  237/ 296 =  80.07%     R =  237/ 291 =  81.44%     F1 =  80.75%\n",
      "Entity-Destination(e2,e1) :    P =    0/   0 =   0.00%     R =    0/   1 =   0.00%     F1 =   0.00%\n",
      "     Entity-Origin(e1,e2) :    P =  139/ 178 =  78.09%     R =  139/ 211 =  65.88%     F1 =  71.47%\n",
      "     Entity-Origin(e2,e1) :    P =   40/  44 =  90.91%     R =   40/  47 =  85.11%     F1 =  87.91%\n",
      " Instrument-Agency(e1,e2) :    P =    9/  18 =  50.00%     R =    9/  22 =  40.91%     F1 =  45.00%\n",
      " Instrument-Agency(e2,e1) :    P =   88/ 114 =  77.19%     R =   88/ 134 =  65.67%     F1 =  70.97%\n",
      " Member-Collection(e1,e2) :    P =   15/  21 =  71.43%     R =   15/  32 =  46.88%     F1 =  56.60%\n",
      " Member-Collection(e2,e1) :    P =  177/ 216 =  81.94%     R =  177/ 201 =  88.06%     F1 =  84.89%\n",
      "     Message-Topic(e1,e2) :    P =  181/ 217 =  83.41%     R =  181/ 210 =  86.19%     F1 =  84.78%\n",
      "     Message-Topic(e2,e1) :    P =   40/  54 =  74.07%     R =   40/  51 =  78.43%     F1 =  76.19%\n",
      "  Product-Producer(e1,e2) :    P =   86/ 118 =  72.88%     R =   86/ 108 =  79.63%     F1 =  76.11%\n",
      "  Product-Producer(e2,e1) :    P =   85/ 117 =  72.65%     R =   85/ 123 =  69.11%     F1 =  70.83%\n",
      "                   _Other :    P =  223/ 467 =  47.75%     R =  223/ 454 =  49.12%     F1 =  48.43%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1792/2250 =  79.64%     R = 1792/2263 =  79.19%     F1 =  79.42%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  73.16%\tR =  71.01%\tF1 =  71.81%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION IGNORING DIRECTIONALITY >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- skip ACTUAL\n",
      "  C-E | 296    2    0    0    2    1    0    4    2   21 |  328    0  328\n",
      "  C-W |   1  261    2    3    2    3    4    4    0   32 |  312    0  312\n",
      "  C-C |   0    8  162    9    3    0    1    1    1    7 |  192    0  192\n",
      "  E-D |   1    6   10  237    4    2    0    2    0   30 |  292    0  292\n",
      "  E-O |   4    6    3   17  179    1    1    3   10   34 |  258    0  258\n",
      "  I-A |   1    6    0    1    0   97    0    2   16   33 |  156    0  156\n",
      "  M-C |   1    5    1    0    1    2  192    1    0   30 |  233    0  233\n",
      "  M-T |   3    2    0    1    1    0    1  222    1   30 |  261    0  261\n",
      "  P-P |   4    3    0    2    7   12    3    1  172   27 |  231    0  231\n",
      "  _O_ |  23   33   13   26   23   14   35   31   33  223 |  454    0  454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  334  332  191  296  222  132  237  271  235  467   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2041/2717 = 75.12%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2041/2717 = 75.12%\n",
      "Accuracy (considering all skipped examples as Other) = 2041/2717 = 75.12%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  296/ 334 =  88.62%     R =  296/ 328 =  90.24%     F1 =  89.43%\n",
      "          Component-Whole :    P =  261/ 332 =  78.61%     R =  261/ 312 =  83.65%     F1 =  81.06%\n",
      "        Content-Container :    P =  162/ 191 =  84.82%     R =  162/ 192 =  84.38%     F1 =  84.60%\n",
      "       Entity-Destination :    P =  237/ 296 =  80.07%     R =  237/ 292 =  81.16%     F1 =  80.61%\n",
      "            Entity-Origin :    P =  179/ 222 =  80.63%     R =  179/ 258 =  69.38%     F1 =  74.58%\n",
      "        Instrument-Agency :    P =   97/ 132 =  73.48%     R =   97/ 156 =  62.18%     F1 =  67.36%\n",
      "        Member-Collection :    P =  192/ 237 =  81.01%     R =  192/ 233 =  82.40%     F1 =  81.70%\n",
      "            Message-Topic :    P =  222/ 271 =  81.92%     R =  222/ 261 =  85.06%     F1 =  83.46%\n",
      "         Product-Producer :    P =  172/ 235 =  73.19%     R =  172/ 231 =  74.46%     F1 =  73.82%\n",
      "                   _Other :    P =  223/ 467 =  47.75%     R =  223/ 454 =  49.12%     F1 =  48.43%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1818/2250 =  80.80%     R = 1818/2263 =  80.34%     F1 =  80.57%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  80.26%\tR =  79.21%\tF1 =  79.62%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- xDIRx skip  ACTUAL\n",
      "  C-E | 288    2    0    0    2    1    0    4    2   21 |  320     8     0    328\n",
      "  C-W |   1  248    2    3    2    3    4    4    0   32 |  299    13     0    312\n",
      "  C-C |   0    8  159    9    3    0    1    1    1    7 |  189     3     0    192\n",
      "  E-D |   1    6   10  237    4    2    0    2    0   30 |  292     0     0    292\n",
      "  E-O |   4    6    3   17  179    1    1    3   10   34 |  258     0     0    258\n",
      "  I-A |   1    6    0    1    0   97    0    2   16   33 |  156     0     0    156\n",
      "  M-C |   1    5    1    0    1    2  192    1    0   30 |  233     0     0    233\n",
      "  M-T |   3    2    0    1    1    0    1  221    1   30 |  260     1     0    261\n",
      "  P-P |   4    3    0    2    7   12    3    1  171   27 |  230     1     0    231\n",
      "  _O_ |  23   33   13   26   23   14   35   31   33  223 |  454     0     0    454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  326  319  188  296  222  132  237  270  234  467   2691    26     0   2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2015/2717 = 74.16%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2015/2717 = 74.16%\n",
      "Accuracy (considering all skipped examples as Other) = 2015/2717 = 74.16%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  288/( 326 +   8) =  86.23%     R =  288/ 328 =  87.80%     F1 =  87.01%\n",
      "          Component-Whole :    P =  248/( 319 +  13) =  74.70%     R =  248/ 312 =  79.49%     F1 =  77.02%\n",
      "        Content-Container :    P =  159/( 188 +   3) =  83.25%     R =  159/ 192 =  82.81%     F1 =  83.03%\n",
      "       Entity-Destination :    P =  237/( 296 +   0) =  80.07%     R =  237/ 292 =  81.16%     F1 =  80.61%\n",
      "            Entity-Origin :    P =  179/( 222 +   0) =  80.63%     R =  179/ 258 =  69.38%     F1 =  74.58%\n",
      "        Instrument-Agency :    P =   97/( 132 +   0) =  73.48%     R =   97/ 156 =  62.18%     F1 =  67.36%\n",
      "        Member-Collection :    P =  192/( 237 +   0) =  81.01%     R =  192/ 233 =  82.40%     F1 =  81.70%\n",
      "            Message-Topic :    P =  221/( 270 +   1) =  81.55%     R =  221/ 261 =  84.67%     F1 =  83.08%\n",
      "         Product-Producer :    P =  171/( 234 +   1) =  72.77%     R =  171/ 231 =  74.03%     F1 =  73.39%\n",
      "                   _Other :    P =  223/( 467 +   0) =  47.75%     R =  223/ 454 =  49.12%     F1 =  48.43%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1792/2250 =  79.64%     R = 1792/2263 =  79.19%     F1 =  79.42%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  79.30%\tR =  78.21%\tF1 =  78.64%\n",
      "\n",
      "\n",
      "\n",
      "<<< The official score is (9+1)-way evaluation with directionality taken into account: macro-averaged F1 = 78.64% >>>\n",
      "/root/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../results\n",
    "!perl ../results/semeval2010_task8_scorer-v1.2.pl model_frozen_bert.txt test_keys.txt\n",
    "%cd ../notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.2511667927913368, 0.4354488536622375, 0.16860813486156986, 0.06277294257859467, 0.02286836014711298], 'accuracy': [0.6173437, 0.85984373, 0.9471875, 0.9821875, 0.9948437], 'val_loss': [0.6425678587704897, 0.5604663681983948, 0.6542311323434115, 0.7178658551722765, 0.6972057731822133], 'val_accuracy': [0.7875, 0.811875, 0.829375, 0.8325, 0.83625], 'lr': [3e-05, 3e-05, 3e-05, 3e-05, 2.9999999e-06]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
