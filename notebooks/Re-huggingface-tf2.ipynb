{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction (SemEval task 8) with Huggingface transformers & TF2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task - predict semantic relations between pairs of nominals.<br>\n",
    "**There are 9 (directional) types:**\n",
    "- Cause-Effect\n",
    "- Component-Whole\n",
    "- Content-Container\n",
    "- Entity-Destination\n",
    "- Entity-Origin\n",
    "- Instrument-Agency\n",
    "- Member-Collection\n",
    "- Message-Topic\n",
    "- Product-Producer\n",
    "- *Other* (none of above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semeval_processor\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/tuned_bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(text):\n",
    "    encoder = semeval_processor.Semeval()\n",
    "    tokens_in = encoder.re_encode(input_text=text,\n",
    "                                  tokenizer=tokenizer)\n",
    "    pred = model.predict(tokens_in)\n",
    "    \n",
    "    return encoder.re_decode(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find RE between 2 entities in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.6 ms, sys: 0 ns, total: 75.6 ms\n",
      "Wall time: 62 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Other'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The <e1>bengal tiger</e1> is the largest of <e2>animals</e2>\"\n",
    "\n",
    "%time get_relation(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SemEval-2010 Task 8 dataset: <br>\n",
    "contains 8000 train and 2717 test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import semeval_processor\n",
    "from utils import features_from_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames for the input and tmp data:\n",
    "train_original = '../data/TRAIN_FILE.TXT'\n",
    "test_original = '../data/TEST_FILE.txt'\n",
    "\n",
    "train_tsv = '../data/train_m.tsv'\n",
    "test_tsv = '../data/test.tsv'\n",
    "\n",
    "pickle_features = '../data/train_features_m'\n",
    "pickle_test = '../data/test_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features_from_tsv(train_tsv, tokenizer)\n",
    "test_features = features_from_tsv(test_tsv, tokenizer, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens:\t\t ['[SEP]', '[MASK]', '</e1>', '<e1>', '[CLS]', '[PAD]', '</e2>', '[UNK]', '<e2>']\n",
      "Spesial token ids:\t [102, 103, 30523, 30522, 101, 0, 30525, 100, 30524] \n",
      "\n",
      "Text example:\t\t The <e1>lawsonite</e1> was contained in a <e2>platinum crucible</e2> and the counter-weight was a plastic crucible with metal pieces. \n",
      "\n",
      "Tokenized example:\t ['the', '<e1>', 'lawson', '##ite', '</e1>', 'was', 'contained', 'in', 'a', '<e2>', 'platinum', 'cr', '##ucible', '</e2>', 'and', 'the', 'counter', '-', 'weight', 'was', 'a', 'plastic', 'cr', '##ucible', 'with', 'metal', 'pieces', '.'] \n",
      "\n",
      "Input ids: \n",
      " [  101  1996 30522 14577  4221 30523  2001  4838  1999  1037 30524  8899\n",
      " 13675 21104 30525  1998  1996  4675  1011  3635  2001  1037  6081 13675\n",
      " 21104  2007  3384  4109  1012   102     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "Attention mask: \n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "Entity1 mask: \n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "Entity2 mask: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Label id: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Special tokens:\\t\\t {tokenizer.all_special_tokens}\")\n",
    "print(f\"Spesial token ids:\\t {tokenizer.all_special_ids} \\n\")\n",
    "\n",
    "example = \"The <e1>lawsonite</e1> was contained in a <e2>platinum crucible</e2> and the counter-weight was a plastic crucible with metal pieces.\"\n",
    "print(f\"Text example:\\t\\t {example} \\n\")\n",
    "print(f\"Tokenized example:\\t {tokenizer.tokenize(example)} \\n\")\n",
    "print(f\"Input ids: \\n {train_features['input_ids'][8]}\\n\")\n",
    "print(f\"Attention mask: \\n {train_features['attention_mask'][8]} \\n\")\n",
    "print(f\"Entity1 mask: \\n {train_features['e1_mask'][8]} \\n\")\n",
    "print(f\"Entity2 mask: \\n {train_features['e2_mask'][8]}\\n\")\n",
    "print(f\"Label id: {train_features['label_id'][8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle and unpickle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the train and test features to pickle: \n",
    "pickle.dump(train_features,open(pickle_features,'wb'))\n",
    "pickle.dump(test_features,open(pickle_test,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read train and test features from pickle\n",
    "train_features = pickle.load(open(pickle_features,'rb'))\n",
    "test_features = pickle.load(open(pickle_test,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ids, xtest_ids, xtrain_mask, xtest_mask, xtrain_e1, xtest_e1, xtrain_e2, xtest_e2, ytrain, ytest = train_test_split(\n",
    "    train_features['input_ids'],\n",
    "    train_features['attention_mask'],\n",
    "    train_features['e1_mask'],\n",
    "    train_features['e2_mask'],\n",
    "    train_features['label_id'],\n",
    "    stratify = train_features['label_id'],\n",
    "    test_size=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "Let's get BERT base uncased model and try to fine-tune it to our downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'bert-base-uncased'\n",
    "config = BertConfig()\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFBertModel.from_pretrained(pretrained_model, config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom layer to extract the embeddings vectors of our marked entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Extract_entity(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Extract_entity, self).__init__()\n",
    "    \n",
    "    def call(self, sequence_output, e_mask):\n",
    "        extended_e_mask = tf.expand_dims(e_mask, 1) # shape (batch_size, 1, sequence_length)\n",
    "        extended_e_mask = tf.cast(extended_e_mask, tf.float32)\n",
    "        ext_entity = tf.matmul(extended_e_mask, sequence_output) # shape (batch_size, 1, hidden_size)\n",
    "        \n",
    "        return tf.squeeze(ext_entity, [1]) # shape (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup tensorboard for watching some nice pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = \"../.logs\"\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    import os\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and add callbacks for early stopping, and reducing learning rate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "lr_on_pla = tf.keras.callbacks.ReduceLROnPlateau(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned BERT model\n",
    "\n",
    "Let's use embeddings (the top layer) from the pretrained hugginface Bert and add dense layrs on top, as in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_tuned(dropout_rate):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(128,), name='e1_mask', dtype='int32')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(128,), name='e2_mask', dtype='int32')\n",
    "\n",
    "    embedding_layer = transformer_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_h = embedding_layer[:,0,:]\n",
    "    e1_h = Extract_entity()(embedding_layer,e1_mask)\n",
    "    e2_h = Extract_entity()(embedding_layer,e2_mask)\n",
    "    CLS = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='cls_dense')(cls_h)\n",
    "    CLS = tf.keras.layers.Dropout(dropout_rate)(CLS)\n",
    "    entity_layer = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='ent')\n",
    "    E1 = entity_layer(e1_h)\n",
    "    E1 = tf.keras.layers.Dropout(dropout_rate)(E1)\n",
    "    E2 = entity_layer(e2_h)\n",
    "    E2 = tf.keras.layers.Dropout(dropout_rate)(E2)\n",
    "    X = tf.concat([CLS,E1,E2],-1)\n",
    "    X = tf.keras.layers.Dense(config.hidden_size*3, activation=\"elu\", kernel_initializer=\"he_normal\")(X)\n",
    "    X = tf.keras.layers.Dropout(dropout_rate)(X)\n",
    "    X = tf.keras.layers.Dense(19, activation='softmax')(X)\n",
    "    model = tf.keras.Model(inputs=[input_ids,attention_mask,e1_mask,e2_mask], outputs=[X])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "e1_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e2_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_2 (Extract_entit (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_3 (Extract_entit (None, 768)          0           tf_bert_model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense (Dense)               (None, 768)          590592      tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "ent (Dense)                     (None, 768)          590592      extract_entity_2[0][0]           \n",
      "                                                                 extract_entity_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 768)          0           cls_dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 768)          0           ent[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 768)          0           ent[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 2304)]       0           dropout_41[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2304)         5310720     tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 2304)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 19)           43795       dropout_44[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 116,017,939\n",
      "Trainable params: 6,535,699\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tuned_bert = create_model_tuned(dropout_rate=0.2)\n",
    "model_tuned_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1601 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "  16/6400 [..............................] - ETA: 1:06:30 - loss: 4.1324 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.949269). Check your callbacks.\n",
      "6400/6400 [==============================] - 185s 29ms/sample - loss: 1.2393 - accuracy: 0.6147 - val_loss: 0.6283 - val_accuracy: 0.8145\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - 171s 27ms/sample - loss: 0.4164 - accuracy: 0.8708 - val_loss: 0.6215 - val_accuracy: 0.8207\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 170s 27ms/sample - loss: 0.1770 - accuracy: 0.9425 - val_loss: 0.7194 - val_accuracy: 0.8301\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 170s 27ms/sample - loss: 0.0687 - accuracy: 0.9792 - val_loss: 0.7170 - val_accuracy: 0.8395\n",
      "Epoch 5/100\n",
      "6384/6400 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9939Restoring model weights from the end of the best epoch.\n",
      "6400/6400 [==============================] - 171s 27ms/sample - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.7249 - val_accuracy: 0.8445\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model_tuned_bert.fit([xtrain_ids, xtrain_mask, xtrain_e1, xtrain_e2],\n",
    "                   ytrain,\n",
    "                   validation_data = ([xtest_ids, xtest_mask, xtest_e1, xtest_e2], ytest),\n",
    "                   epochs=100,\n",
    "                   batch_size=16,\n",
    "                   callbacks=[early_stop,\n",
    "                             lr_on_pla,\n",
    "                             tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/tuned_bert/assets\n"
     ]
    }
   ],
   "source": [
    "model_tuned_bert.save('../models/tuned_bert',save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[test Tuned Bert](#3.-Evaluate-Models)\n",
    "\n",
    "### Frozen BERT model\n",
    "\n",
    "Let's use BERT pre-trained model as the feature extractor by freezing Bert's output (embeddings). <br>\n",
    "In this way we'll be training only added dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_frozen(dropout_rate):\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n",
    "    e1_mask = tf.keras.layers.Input(shape=(128,), name='e1_mask', dtype='int32')\n",
    "    e2_mask = tf.keras.layers.Input(shape=(128,), name='e2_mask', dtype='int32')\n",
    "\n",
    "    embedding_layer = transformer_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_h = embedding_layer[:,0,:]\n",
    "    e1_h = Extract_entity()(embedding_layer,e1_mask)\n",
    "    e2_h = Extract_entity()(embedding_layer,e2_mask)\n",
    "    CLS = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='cls_dense')(cls_h)\n",
    "    CLS = tf.keras.layers.Dropout(dropout_rate)(CLS)\n",
    "    entity_layer = tf.keras.layers.Dense(config.hidden_size, activation=\"elu\", kernel_initializer=\"he_normal\", name='ent')\n",
    "    E1 = entity_layer(e1_h)\n",
    "    E1 = tf.keras.layers.Dropout(dropout_rate)(E1)\n",
    "    E2 = entity_layer(e2_h)\n",
    "    E2 = tf.keras.layers.Dropout(dropout_rate)(E2)\n",
    "    X = tf.concat([CLS,E1,E2],-1)\n",
    "    X = tf.keras.layers.Dense(config.hidden_size*3, activation=\"elu\", kernel_initializer=\"he_normal\")(X)\n",
    "    X = tf.keras.layers.Dropout(dropout_rate)(X)\n",
    "    X = tf.keras.layers.Dense(19, activation='softmax')(X)\n",
    "    model = tf.keras.Model(inputs=[input_ids,attention_mask,e1_mask,e2_mask], outputs=[X])\n",
    "\n",
    "    #don't mess up BERT layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "e1_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e2_mask (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity (Extract_entity) (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "extract_entity_1 (Extract_entit (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense (Dense)               (None, 768)          590592      tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "ent (Dense)                     (None, 768)          590592      extract_entity[0][0]             \n",
      "                                                                 extract_entity_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 768)          0           cls_dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 768)          0           ent[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 768)          0           ent[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 2304)]       0           dropout_37[0][0]                 \n",
      "                                                                 dropout_38[0][0]                 \n",
      "                                                                 dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2304)         5310720     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 2304)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 19)           43795       dropout_40[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 116,017,939\n",
      "Trainable params: 6,535,699\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_frozen_bert = create_model_frozen(dropout_rate=0.1)\n",
    "model_frozen_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1601 samples\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/100\n",
      "  16/6400 [..............................] - ETA: 41:00 - loss: 3.9300 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.186292). Check your callbacks.\n",
      "6400/6400 [==============================] - 76s 12ms/sample - loss: 2.0058 - accuracy: 0.3797 - val_loss: 1.2988 - val_accuracy: 0.6127\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 1.2211 - accuracy: 0.6028 - val_loss: 1.0189 - val_accuracy: 0.6733\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.9600 - accuracy: 0.6858 - val_loss: 0.9083 - val_accuracy: 0.7096\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.8277 - accuracy: 0.7330 - val_loss: 0.8646 - val_accuracy: 0.7258\n",
      "Epoch 5/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.7185 - accuracy: 0.7627 - val_loss: 0.8290 - val_accuracy: 0.7358\n",
      "Epoch 6/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.6369 - accuracy: 0.7895 - val_loss: 0.8137 - val_accuracy: 0.7339\n",
      "Epoch 7/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.5958 - accuracy: 0.7961 - val_loss: 0.8172 - val_accuracy: 0.7452\n",
      "Epoch 8/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.5422 - accuracy: 0.8194 - val_loss: 0.8093 - val_accuracy: 0.7458\n",
      "Epoch 9/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.4829 - accuracy: 0.8378 - val_loss: 0.7854 - val_accuracy: 0.7458\n",
      "Epoch 10/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.4688 - accuracy: 0.8386 - val_loss: 0.7966 - val_accuracy: 0.7577\n",
      "Epoch 11/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.4329 - accuracy: 0.8537 - val_loss: 0.7953 - val_accuracy: 0.7514\n",
      "Epoch 12/100\n",
      "6400/6400 [==============================] - 68s 11ms/sample - loss: 0.3708 - accuracy: 0.8784 - val_loss: 0.7832 - val_accuracy: 0.7583\n",
      "Epoch 13/100\n",
      "5760/6400 [==========================>...] - ETA: 5s - loss: 0.3540 - accuracy: 0.8861"
     ]
    }
   ],
   "source": [
    "history = model_frozen_bert.fit([xtrain_ids, xtrain_mask, xtrain_e1, xtrain_e2],\n",
    "                   ytrain,\n",
    "                   validation_data = ([xtest_ids, xtest_mask, xtest_e1, xtest_e2], ytest),\n",
    "                   epochs=100,\n",
    "                   batch_size=16,\n",
    "                   callbacks=[early_stop,\n",
    "                             lr_on_pla,\n",
    "                             tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/frozen_bert/assets\n"
     ]
    }
   ],
   "source": [
    "model_frozen_bert.save('../models/frozen_bert',save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate Models\n",
    "\n",
    "get results for the test set and store in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_semeval_result(result_file, prediction):\n",
    "\n",
    "    res = [semeval_processor.Semeval().RELATION_LABELS[x.argmax()] for x in prediction]\n",
    "\n",
    "    with open(result_file, \"w\") as f:\n",
    "        for i, answer in enumerate(res):\n",
    "            f.write(f\"{8001+i}\\t{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned BERT\n",
    "pred_res = model.predict([\n",
    "                            test_features['input_ids'],\n",
    "                            test_features['attention_mask'],\n",
    "                            test_features['e1_mask'],\n",
    "                            test_features['e2_mask']\n",
    "                            ])\n",
    "\n",
    "write_semeval_result('../results/model_tuned_bert.txt', pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frozen BERT\n",
    "pred_res = model_frozen_bert.predict([\n",
    "                            test_features['input_ids'],\n",
    "                            test_features['attention_mask'],\n",
    "                            test_features['e1_mask'],\n",
    "                            test_features['e2_mask']\n",
    "                            ])\n",
    "\n",
    "write_semeval_result('../results/model_frozen_bert.txt', pred_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the outcome \n",
    "\n",
    "using original evaluation script (looong and detailed)\n",
    "\n",
    "#### Tuned BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/results\n",
      "<<< (2*9+1)-WAY EVALUATION (USING DIRECTIONALITY)>>>:\n",
      "\n",
      "Confusion matrix:\n",
      "        C-E1 C-E2 C-W1 C-W2 C-C1 C-C2 E-D1 E-D2 E-O1 E-O2 I-A1 I-A2 M-C1 M-C2 M-T1 M-T2 P-P1 P-P2  _O_ <-- classified as\n",
      "      +-----------------------------------------------------------------------------------------------+ -SUM- skip ACTUAL\n",
      " C-E1 | 128    1    0    0    0    0    0    0    0    0    0    0    0    0    2    0    0    2    1 |  134    0  134\n",
      " C-E2 |   0  183    0    0    0    0    0    0    3    0    0    0    0    0    0    0    0    0    8 |  194    0  194\n",
      " C-W1 |   0    0  141    7    1    0    0    0    0    0    1    0    1    2    1    0    0    1    7 |  162    0  162\n",
      " C-W2 |   0    0    3  122    0    1    0    0    1    0    0    7    0    2    4    0    0    3    7 |  150    0  150\n",
      " C-C1 |   0    0    6    1  127    0   13    0    1    0    0    0    0    0    0    0    0    0    5 |  153    0  153\n",
      " C-C2 |   0    0    0    4    0   32    0    0    0    1    0    0    0    1    0    0    0    1    0 |   39    0   39\n",
      " E-D1 |   0    0    3    0    0    0  272    0    0    0    0    0    0    0    1    2    0    0   13 |  291    0  291\n",
      " E-D2 |   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1 |    1    0    1\n",
      " E-O1 |   0    5    5    1    1    0    3    0  171    0    0    1    0    0    0    1    4    0   19 |  211    0  211\n",
      " E-O2 |   2    0    0    0    0    0    0    0    0   40    0    0    0    0    0    0    0    4    1 |   47    0   47\n",
      " I-A1 |   0    0    0    0    0    0    0    0    0    0   14    0    0    0    0    0    1    0    7 |   22    0   22\n",
      " I-A2 |   0    0    0    4    0    0    0    0    0    0    0  110    0    0    2    0    0    4   14 |  134    0  134\n",
      " M-C1 |   0    0    0    0    0    0    1    0    1    0    0    0   21    1    0    1    0    1    6 |   32    0   32\n",
      " M-C2 |   0    0    2    0    0    0    0    0    0    0    0    1    0  178    0    0    0    1   19 |  201    0  201\n",
      " M-T1 |   0    1    0    0    0    0    0    0    0    0    0    0    0    1  203    0    0    0    5 |  210    0  210\n",
      " M-T2 |   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   51    0    0    0 |   51    0   51\n",
      " P-P1 |   0    4    0    0    0    0    0    0    3    0    0    0    0    0    1    0   94    1    5 |  108    0  108\n",
      " P-P2 |   1    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0  118    3 |  123    0  123\n",
      "  _O_ |  11   13   23   13   10    2   16    0   15    3    2   17    3   18   37   12   10   21  228 |  454    0  454\n",
      "      +-----------------------------------------------------------------------------------------------+\n",
      " -SUM-  142  207  183  152  139   35  305    0  195   44   17  137   25  203  251   67  109  157  349   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Other) = 2233/2717 = 82.19%\n",
      "\n",
      "Results for the individual relations:\n",
      "      Cause-Effect(e1,e2) :    P =  128/ 142 =  90.14%     R =  128/ 134 =  95.52%     F1 =  92.75%\n",
      "      Cause-Effect(e2,e1) :    P =  183/ 207 =  88.41%     R =  183/ 194 =  94.33%     F1 =  91.27%\n",
      "   Component-Whole(e1,e2) :    P =  141/ 183 =  77.05%     R =  141/ 162 =  87.04%     F1 =  81.74%\n",
      "   Component-Whole(e2,e1) :    P =  122/ 152 =  80.26%     R =  122/ 150 =  81.33%     F1 =  80.79%\n",
      " Content-Container(e1,e2) :    P =  127/ 139 =  91.37%     R =  127/ 153 =  83.01%     F1 =  86.99%\n",
      " Content-Container(e2,e1) :    P =   32/  35 =  91.43%     R =   32/  39 =  82.05%     F1 =  86.49%\n",
      "Entity-Destination(e1,e2) :    P =  272/ 305 =  89.18%     R =  272/ 291 =  93.47%     F1 =  91.28%\n",
      "Entity-Destination(e2,e1) :    P =    0/   0 =   0.00%     R =    0/   1 =   0.00%     F1 =   0.00%\n",
      "     Entity-Origin(e1,e2) :    P =  171/ 195 =  87.69%     R =  171/ 211 =  81.04%     F1 =  84.24%\n",
      "     Entity-Origin(e2,e1) :    P =   40/  44 =  90.91%     R =   40/  47 =  85.11%     F1 =  87.91%\n",
      " Instrument-Agency(e1,e2) :    P =   14/  17 =  82.35%     R =   14/  22 =  63.64%     F1 =  71.79%\n",
      " Instrument-Agency(e2,e1) :    P =  110/ 137 =  80.29%     R =  110/ 134 =  82.09%     F1 =  81.18%\n",
      " Member-Collection(e1,e2) :    P =   21/  25 =  84.00%     R =   21/  32 =  65.62%     F1 =  73.68%\n",
      " Member-Collection(e2,e1) :    P =  178/ 203 =  87.68%     R =  178/ 201 =  88.56%     F1 =  88.12%\n",
      "     Message-Topic(e1,e2) :    P =  203/ 251 =  80.88%     R =  203/ 210 =  96.67%     F1 =  88.07%\n",
      "     Message-Topic(e2,e1) :    P =   51/  67 =  76.12%     R =   51/  51 = 100.00%     F1 =  86.44%\n",
      "  Product-Producer(e1,e2) :    P =   94/ 109 =  86.24%     R =   94/ 108 =  87.04%     F1 =  86.64%\n",
      "  Product-Producer(e2,e1) :    P =  118/ 157 =  75.16%     R =  118/ 123 =  95.93%     F1 =  84.29%\n",
      "                   _Other :    P =  228/ 349 =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2005/2368 =  84.67%     R = 2005/2263 =  88.60%     F1 =  86.59%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  79.95%\tR =  81.25%\tF1 =  80.20%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION IGNORING DIRECTIONALITY >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- skip ACTUAL\n",
      "  C-E | 312    0    0    0    3    0    0    2    2    9 |  328    0  328\n",
      "  C-W |   0  273    2    0    1    8    5    5    4   14 |  312    0  312\n",
      "  C-C |   0   11  159   13    2    0    1    0    1    5 |  192    0  192\n",
      "  E-D |   0    3    0  272    0    0    0    3    0   14 |  292    0  292\n",
      "  E-O |   7    6    1    3  211    1    0    1    8   20 |  258    0  258\n",
      "  I-A |   0    4    0    0    0  124    0    2    5   21 |  156    0  156\n",
      "  M-C |   0    2    0    1    1    1  200    1    2   25 |  233    0  233\n",
      "  M-T |   1    0    0    0    0    0    1  254    0    5 |  261    0  261\n",
      "  P-P |   5    0    0    0    3    1    0    1  213    8 |  231    0  231\n",
      "  _O_ |  24   36   12   16   18   19   21   49   31  228 |  454    0  454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  349  335  174  305  239  154  228  318  266  349   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2246/2717 = 82.66%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2246/2717 = 82.66%\n",
      "Accuracy (considering all skipped examples as Other) = 2246/2717 = 82.66%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  312/ 349 =  89.40%     R =  312/ 328 =  95.12%     F1 =  92.17%\n",
      "          Component-Whole :    P =  273/ 335 =  81.49%     R =  273/ 312 =  87.50%     F1 =  84.39%\n",
      "        Content-Container :    P =  159/ 174 =  91.38%     R =  159/ 192 =  82.81%     F1 =  86.89%\n",
      "       Entity-Destination :    P =  272/ 305 =  89.18%     R =  272/ 292 =  93.15%     F1 =  91.12%\n",
      "            Entity-Origin :    P =  211/ 239 =  88.28%     R =  211/ 258 =  81.78%     F1 =  84.91%\n",
      "        Instrument-Agency :    P =  124/ 154 =  80.52%     R =  124/ 156 =  79.49%     F1 =  80.00%\n",
      "        Member-Collection :    P =  200/ 228 =  87.72%     R =  200/ 233 =  85.84%     F1 =  86.77%\n",
      "            Message-Topic :    P =  254/ 318 =  79.87%     R =  254/ 261 =  97.32%     F1 =  87.74%\n",
      "         Product-Producer :    P =  213/ 266 =  80.08%     R =  213/ 231 =  92.21%     F1 =  85.71%\n",
      "                   _Other :    P =  228/ 349 =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2018/2368 =  85.22%     R = 2018/2263 =  89.17%     F1 =  87.15%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  85.32%\tR =  88.36%\tF1 =  86.63%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- xDIRx skip  ACTUAL\n",
      "  C-E | 311    0    0    0    3    0    0    2    2    9 |  327     1     0    328\n",
      "  C-W |   0  263    2    0    1    8    5    5    4   14 |  302    10     0    312\n",
      "  C-C |   0   11  159   13    2    0    1    0    1    5 |  192     0     0    192\n",
      "  E-D |   0    3    0  272    0    0    0    3    0   14 |  292     0     0    292\n",
      "  E-O |   7    6    1    3  211    1    0    1    8   20 |  258     0     0    258\n",
      "  I-A |   0    4    0    0    0  124    0    2    5   21 |  156     0     0    156\n",
      "  M-C |   0    2    0    1    1    1  199    1    2   25 |  232     1     0    233\n",
      "  M-T |   1    0    0    0    0    0    1  254    0    5 |  261     0     0    261\n",
      "  P-P |   5    0    0    0    3    1    0    1  212    8 |  230     1     0    231\n",
      "  _O_ |  24   36   12   16   18   19   21   49   31  228 |  454     0     0    454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  348  325  174  305  239  154  227  318  265  349   2704    13     0   2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2233/2717 = 82.19%\n",
      "Accuracy (considering all skipped examples as Other) = 2233/2717 = 82.19%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  311/( 348 +   1) =  89.11%     R =  311/ 328 =  94.82%     F1 =  91.88%\n",
      "          Component-Whole :    P =  263/( 325 +  10) =  78.51%     R =  263/ 312 =  84.29%     F1 =  81.30%\n",
      "        Content-Container :    P =  159/( 174 +   0) =  91.38%     R =  159/ 192 =  82.81%     F1 =  86.89%\n",
      "       Entity-Destination :    P =  272/( 305 +   0) =  89.18%     R =  272/ 292 =  93.15%     F1 =  91.12%\n",
      "            Entity-Origin :    P =  211/( 239 +   0) =  88.28%     R =  211/ 258 =  81.78%     F1 =  84.91%\n",
      "        Instrument-Agency :    P =  124/( 154 +   0) =  80.52%     R =  124/ 156 =  79.49%     F1 =  80.00%\n",
      "        Member-Collection :    P =  199/( 227 +   1) =  87.28%     R =  199/ 233 =  85.41%     F1 =  86.33%\n",
      "            Message-Topic :    P =  254/( 318 +   0) =  79.87%     R =  254/ 261 =  97.32%     F1 =  87.74%\n",
      "         Product-Producer :    P =  212/( 265 +   1) =  79.70%     R =  212/ 231 =  91.77%     F1 =  85.31%\n",
      "                   _Other :    P =  228/( 349 +   0) =  65.33%     R =  228/ 454 =  50.22%     F1 =  56.79%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 2005/2368 =  84.67%     R = 2005/2263 =  88.60%     F1 =  86.59%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  84.87%\tR =  87.87%\tF1 =  86.16%\n",
      "\n",
      "\n",
      "\n",
      "<<< The official score is (9+1)-way evaluation with directionality taken into account: macro-averaged F1 = 86.16% >>>\n",
      "/root/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../results\n",
    "!perl ../results/semeval2010_task8_scorer-v1.2.pl model_tuned_bert.txt test_keys.txt\n",
    "%cd ../notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to models](#Frozen-BERT-model)\n",
    "\n",
    "#### Frozen BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/results\n",
      "<<< (2*9+1)-WAY EVALUATION (USING DIRECTIONALITY)>>>:\n",
      "\n",
      "Confusion matrix:\n",
      "        C-E1 C-E2 C-W1 C-W2 C-C1 C-C2 E-D1 E-D2 E-O1 E-O2 I-A1 I-A2 M-C1 M-C2 M-T1 M-T2 P-P1 P-P2  _O_ <-- classified as\n",
      "      +-----------------------------------------------------------------------------------------------+ -SUM- skip ACTUAL\n",
      " C-E1 | 119    4    0    1    0    0    0    0    0    0    0    0    0    0    2    0    0    2    6 |  134    0  134\n",
      " C-E2 |   2  173    1    0    0    0    0    0    4    0    0    0    0    0    0    1    1    0   12 |  194    0  194\n",
      " C-W1 |   0    0  136    4    0    0    3    0    1    0    1    0    0    3    1    0    0    0   13 |  162    0  162\n",
      " C-W2 |   0    0    6  111    0    2    1    0    2    0    0    5    0    1    5    0    0    0   17 |  150    0  150\n",
      " C-C1 |   0    0    6    1  127    1    9    0    2    0    0    0    0    0    0    0    1    0    6 |  153    0  153\n",
      " C-C2 |   0    0    0    2    3   30    0    0    0    1    0    0    0    1    0    0    0    0    2 |   39    0   39\n",
      " E-D1 |   0    1    6    1    9    0  239    0    5    0    0    2    0    0    0    0    1    0   27 |  291    0  291\n",
      " E-D2 |   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1 |    1    0    1\n",
      " E-O1 |   0    7    1    3    3    0   14    0  144    0    0    1    2    0    1    1    8    0   26 |  211    0  211\n",
      " E-O2 |   1    0    0    0    0    0    1    0    0   37    0    0    0    0    0    0    0    2    6 |   47    0   47\n",
      " I-A1 |   0    0    0    0    0    0    0    0    0    0   11    0    0    1    0    0    2    0    8 |   22    0   22\n",
      " I-A2 |   1    0    0    5    0    0    1    0    1    0    0   92    0    0    3    0    0    6   25 |  134    0  134\n",
      " M-C1 |   0    0    2    0    0    0    1    0    1    0    0    2   16    1    0    1    0    0    8 |   32    0   32\n",
      " M-C2 |   0    1    2    2    0    1    0    0    1    0    2    0    0  172    1    0    0    0   19 |  201    0  201\n",
      " M-T1 |   0    3    1    4    0    0    0    0    0    0    0    0    0    1  184    0    0    0   17 |  210    0  210\n",
      " M-T2 |   0    0    1    0    0    0    0    0    2    0    0    0    0    0    1   38    0    1    8 |   51    0   51\n",
      " P-P1 |   0    4    0    0    0    0    1    0    3    0    3    0    0    2    0    0   87    1    7 |  108    0  108\n",
      " P-P2 |   0    0    1    3    0    0    1    0    1    1    0    6    0    1    0    0    0   90   19 |  123    0  123\n",
      "  _O_ |  13   13   14   17   11    5   26    0   20    2    0   15    6   26   22    7   14   16  227 |  454    0  454\n",
      "      +-----------------------------------------------------------------------------------------------+\n",
      " -SUM-  136  206  177  154  153   39  297    0  187   41   17  123   24  209  220   48  114  118  454   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2033/2717 = 74.83%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2033/2717 = 74.83%\n",
      "Accuracy (considering all skipped examples as Other) = 2033/2717 = 74.83%\n",
      "\n",
      "Results for the individual relations:\n",
      "      Cause-Effect(e1,e2) :    P =  119/ 136 =  87.50%     R =  119/ 134 =  88.81%     F1 =  88.15%\n",
      "      Cause-Effect(e2,e1) :    P =  173/ 206 =  83.98%     R =  173/ 194 =  89.18%     F1 =  86.50%\n",
      "   Component-Whole(e1,e2) :    P =  136/ 177 =  76.84%     R =  136/ 162 =  83.95%     F1 =  80.24%\n",
      "   Component-Whole(e2,e1) :    P =  111/ 154 =  72.08%     R =  111/ 150 =  74.00%     F1 =  73.03%\n",
      " Content-Container(e1,e2) :    P =  127/ 153 =  83.01%     R =  127/ 153 =  83.01%     F1 =  83.01%\n",
      " Content-Container(e2,e1) :    P =   30/  39 =  76.92%     R =   30/  39 =  76.92%     F1 =  76.92%\n",
      "Entity-Destination(e1,e2) :    P =  239/ 297 =  80.47%     R =  239/ 291 =  82.13%     F1 =  81.29%\n",
      "Entity-Destination(e2,e1) :    P =    0/   0 =   0.00%     R =    0/   1 =   0.00%     F1 =   0.00%\n",
      "     Entity-Origin(e1,e2) :    P =  144/ 187 =  77.01%     R =  144/ 211 =  68.25%     F1 =  72.36%\n",
      "     Entity-Origin(e2,e1) :    P =   37/  41 =  90.24%     R =   37/  47 =  78.72%     F1 =  84.09%\n",
      " Instrument-Agency(e1,e2) :    P =   11/  17 =  64.71%     R =   11/  22 =  50.00%     F1 =  56.41%\n",
      " Instrument-Agency(e2,e1) :    P =   92/ 123 =  74.80%     R =   92/ 134 =  68.66%     F1 =  71.60%\n",
      " Member-Collection(e1,e2) :    P =   16/  24 =  66.67%     R =   16/  32 =  50.00%     F1 =  57.14%\n",
      " Member-Collection(e2,e1) :    P =  172/ 209 =  82.30%     R =  172/ 201 =  85.57%     F1 =  83.90%\n",
      "     Message-Topic(e1,e2) :    P =  184/ 220 =  83.64%     R =  184/ 210 =  87.62%     F1 =  85.58%\n",
      "     Message-Topic(e2,e1) :    P =   38/  48 =  79.17%     R =   38/  51 =  74.51%     F1 =  76.77%\n",
      "  Product-Producer(e1,e2) :    P =   87/ 114 =  76.32%     R =   87/ 108 =  80.56%     F1 =  78.38%\n",
      "  Product-Producer(e2,e1) :    P =   90/ 118 =  76.27%     R =   90/ 123 =  73.17%     F1 =  74.69%\n",
      "                   _Other :    P =  227/ 454 =  50.00%     R =  227/ 454 =  50.00%     F1 =  50.00%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1806/2263 =  79.81%     R = 1806/2263 =  79.81%     F1 =  79.81%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  73.99%\tR =  71.95%\tF1 =  72.78%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION IGNORING DIRECTIONALITY >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- skip ACTUAL\n",
      "  C-E | 298    2    0    0    4    0    0    3    3   18 |  328    0  328\n",
      "  C-W |   0  257    2    4    3    6    4    6    0   30 |  312    0  312\n",
      "  C-C |   0    9  161    9    3    0    1    0    1    8 |  192    0  192\n",
      "  E-D |   1    7    9  239    5    2    0    0    1   28 |  292    0  292\n",
      "  E-O |   8    4    3   15  181    1    2    2   10   32 |  258    0  258\n",
      "  I-A |   1    5    0    1    1  103    1    3    8   33 |  156    0  156\n",
      "  M-C |   1    6    1    1    2    4  189    2    0   27 |  233    0  233\n",
      "  M-T |   3    6    0    0    2    0    1  223    1   25 |  261    0  261\n",
      "  P-P |   4    4    0    2    5    9    3    0  178   26 |  231    0  231\n",
      "  _O_ |  26   31   16   26   22   15   32   29   30  227 |  454    0  454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  342  331  192  297  228  140  233  268  232  454   2717    0 2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2056/2717 = 75.67%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2056/2717 = 75.67%\n",
      "Accuracy (considering all skipped examples as Other) = 2056/2717 = 75.67%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  298/ 342 =  87.13%     R =  298/ 328 =  90.85%     F1 =  88.96%\n",
      "          Component-Whole :    P =  257/ 331 =  77.64%     R =  257/ 312 =  82.37%     F1 =  79.94%\n",
      "        Content-Container :    P =  161/ 192 =  83.85%     R =  161/ 192 =  83.85%     F1 =  83.85%\n",
      "       Entity-Destination :    P =  239/ 297 =  80.47%     R =  239/ 292 =  81.85%     F1 =  81.15%\n",
      "            Entity-Origin :    P =  181/ 228 =  79.39%     R =  181/ 258 =  70.16%     F1 =  74.49%\n",
      "        Instrument-Agency :    P =  103/ 140 =  73.57%     R =  103/ 156 =  66.03%     F1 =  69.59%\n",
      "        Member-Collection :    P =  189/ 233 =  81.12%     R =  189/ 233 =  81.12%     F1 =  81.12%\n",
      "            Message-Topic :    P =  223/ 268 =  83.21%     R =  223/ 261 =  85.44%     F1 =  84.31%\n",
      "         Product-Producer :    P =  178/ 232 =  76.72%     R =  178/ 231 =  77.06%     F1 =  76.89%\n",
      "                   _Other :    P =  227/ 454 =  50.00%     R =  227/ 454 =  50.00%     F1 =  50.00%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1829/2263 =  80.82%     R = 1829/2263 =  80.82%     F1 =  80.82%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  80.35%\tR =  79.86%\tF1 =  80.03%\n",
      "\n",
      "\n",
      "\n",
      "<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:\n",
      "\n",
      "Confusion matrix:\n",
      "         C-E  C-W  C-C  E-D  E-O  I-A  M-C  M-T  P-P  _O_ <-- classified as\n",
      "      +--------------------------------------------------+ -SUM- xDIRx skip  ACTUAL\n",
      "  C-E | 292    2    0    0    4    0    0    3    3   18 |  322     6     0    328\n",
      "  C-W |   0  247    2    4    3    6    4    6    0   30 |  302    10     0    312\n",
      "  C-C |   0    9  157    9    3    0    1    0    1    8 |  188     4     0    192\n",
      "  E-D |   1    7    9  239    5    2    0    0    1   28 |  292     0     0    292\n",
      "  E-O |   8    4    3   15  181    1    2    2   10   32 |  258     0     0    258\n",
      "  I-A |   1    5    0    1    1  103    1    3    8   33 |  156     0     0    156\n",
      "  M-C |   1    6    1    1    2    4  188    2    0   27 |  232     1     0    233\n",
      "  M-T |   3    6    0    0    2    0    1  222    1   25 |  260     1     0    261\n",
      "  P-P |   4    4    0    2    5    9    3    0  177   26 |  230     1     0    231\n",
      "  _O_ |  26   31   16   26   22   15   32   29   30  227 |  454     0     0    454\n",
      "      +--------------------------------------------------+\n",
      " -SUM-  336  321  188  297  228  140  232  267  231  454   2694    23     0   2717\n",
      "\n",
      "Coverage = 2717/2717 = 100.00%\n",
      "Accuracy (calculated for the above confusion matrix) = 2033/2717 = 74.83%\n",
      "Accuracy (considering all skipped examples as Wrong) = 2033/2717 = 74.83%\n",
      "Accuracy (considering all skipped examples as Other) = 2033/2717 = 74.83%\n",
      "\n",
      "Results for the individual relations:\n",
      "             Cause-Effect :    P =  292/( 336 +   6) =  85.38%     R =  292/ 328 =  89.02%     F1 =  87.16%\n",
      "          Component-Whole :    P =  247/( 321 +  10) =  74.62%     R =  247/ 312 =  79.17%     F1 =  76.83%\n",
      "        Content-Container :    P =  157/( 188 +   4) =  81.77%     R =  157/ 192 =  81.77%     F1 =  81.77%\n",
      "       Entity-Destination :    P =  239/( 297 +   0) =  80.47%     R =  239/ 292 =  81.85%     F1 =  81.15%\n",
      "            Entity-Origin :    P =  181/( 228 +   0) =  79.39%     R =  181/ 258 =  70.16%     F1 =  74.49%\n",
      "        Instrument-Agency :    P =  103/( 140 +   0) =  73.57%     R =  103/ 156 =  66.03%     F1 =  69.59%\n",
      "        Member-Collection :    P =  188/( 232 +   1) =  80.69%     R =  188/ 233 =  80.69%     F1 =  80.69%\n",
      "            Message-Topic :    P =  222/( 267 +   1) =  82.84%     R =  222/ 261 =  85.06%     F1 =  83.93%\n",
      "         Product-Producer :    P =  177/( 231 +   1) =  76.29%     R =  177/ 231 =  76.62%     F1 =  76.46%\n",
      "                   _Other :    P =  227/( 454 +   0) =  50.00%     R =  227/ 454 =  50.00%     F1 =  50.00%\n",
      "\n",
      "Micro-averaged result (excluding Other):\n",
      "P = 1806/2263 =  79.81%     R = 1806/2263 =  79.81%     F1 =  79.81%\n",
      "\n",
      "MACRO-averaged result (excluding Other):\n",
      "P =  79.45%\tR =  78.93%\tF1 =  79.12%\n",
      "\n",
      "\n",
      "\n",
      "<<< The official score is (9+1)-way evaluation with directionality taken into account: macro-averaged F1 = 79.12% >>>\n",
      "/root/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../results\n",
    "!perl ../results/semeval2010_task8_scorer-v1.2.pl model_frozen_bert.txt test_keys.txt\n",
    "%cd ../notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to inference](#Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [2.005833934992552, 1.221050978899002, 0.9599849098920822, 0.8276855429261922, 0.7184876125305891, 0.6368627072870732, 0.595831923186779, 0.5422222079709172, 0.48287706056609747, 0.46883481115102765, 0.43291062824428084, 0.3708237578161061, 0.3543814686872065, 0.35717217094264925, 0.34557474026456475, 0.34826085837557913, 0.34086725528351963, 0.3301636672951281, 0.3368642366863787, 0.3365003706514835, 0.3311690890043974, 0.3230461867339909, 0.33095052948221565, 0.325369396135211, 0.319444582760334], 'accuracy': [0.3796875, 0.6028125, 0.68578124, 0.73296875, 0.7626563, 0.78953123, 0.79609376, 0.819375, 0.8378125, 0.8385937, 0.85375, 0.8784375, 0.8859375, 0.88125, 0.88671875, 0.886875, 0.8915625, 0.894375, 0.89109373, 0.891875, 0.895625, 0.89890623, 0.89484376, 0.8953125, 0.8990625], 'val_loss': [1.2988108759891384, 1.0188710082236265, 0.9082572736325374, 0.8645573751971842, 0.829001322546875, 0.8137138436765391, 0.8172277401060071, 0.8093317065394945, 0.7853702791384203, 0.7966010772078057, 0.7952637128220135, 0.7831800867018663, 0.7779761022893359, 0.7751292001728264, 0.7733238617343354, 0.7771843632288756, 0.7731255226763318, 0.7722854781791167, 0.770307293214104, 0.7705441823160551, 0.7714227365561384, 0.7678028776785644, 0.7680112735172274, 0.768390431185762, 0.7684006555556209], 'val_accuracy': [0.612742, 0.6733292, 0.7095565, 0.7257964, 0.73579013, 0.7339163, 0.74515927, 0.74578387, 0.74578387, 0.75765145, 0.75140536, 0.7582761, 0.75702685, 0.755153, 0.7589007, 0.75640225, 0.7589007, 0.76327294, 0.76327294, 0.76264834, 0.76327294, 0.76389754, 0.7651468, 0.7645222, 0.76389754], 'lr': [3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 3e-05, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999999e-06, 2.9999998e-07, 2.9999998e-07, 2.9999998e-07, 3e-08]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
